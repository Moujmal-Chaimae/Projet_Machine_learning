{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameter Optimization Notebook\n\nThis notebook performs hyperparameter tuning on the best performing model from the training phase.\n\nThe notebook performs the following steps:\n1. Load configuration and best performing model\n2. Load processed training and test data\n3. Define hyperparameter grids from config\n4. Initialize HyperparameterOptimizer\n5. Run hyperparameter optimization (limited to 2 hours max)\n6. Evaluate optimized model on test set\n7. Verify F1-score improvement meets minimum threshold of 0.80\n8. Save optimized model as best_model.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom modules\n",
    "from src.modeling.hyperparameter_optimizer import HyperparameterOptimizer\n",
    "from src.modeling.model_registry import ModelRegistry\n",
    "from src.evaluation.model_evaluator import ModelEvaluator\n",
    "from src.utils.logger import get_logger\n",
    "\n",
    "# Set up logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('All imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from YAML file\n",
    "config_path = '../config/config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print('Configuration loaded successfully!')\n",
    "print(f\"\\nHyperparameter tuning enabled: {config['hyperparameter_tuning']['enabled']}\")\n",
    "print(f\"Search method: {config['hyperparameter_tuning']['method']}\")\n",
    "print(f\"CV folds: {config['hyperparameter_tuning']['cv_folds']}\")\n",
    "print(f\"Number of iterations: {config['hyperparameter_tuning']['n_iter']}\")\n",
    "print(f\"Primary metric: {config['evaluation']['primary_metric']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed training and test data\n",
    "processed_data_path = config['data']['processed_data_path']\n",
    "\n",
    "print(f'Loading data from {processed_data_path}...')\n",
    "\n",
    "# Load datasets\n",
    "with open(f'{processed_data_path}X_train.pkl', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "\n",
    "with open(f'{processed_data_path}X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open(f'{processed_data_path}y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "with open(f'{processed_data_path}y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "print('\\nData loaded successfully!')\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Best Performing Model from Previous Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ModelRegistry\n",
    "registry = ModelRegistry(models_dir='../models')\n",
    "\n",
    "# List all available models\n",
    "print('Available models in registry:')\n",
    "print('=' * 80)\n",
    "models_list = registry.list_models()\n",
    "for model_info in models_list:\n",
    "    print(f\"Model: {model_info['model_name']} v{model_info['version']}\")\n",
    "    if 'metrics' in model_info and model_info['metrics']:\n",
    "        print(f\"  F1-Score: {model_info['metrics'].get('f1_score', 'N/A')}\")\n",
    "        print(f\"  Accuracy: {model_info['metrics'].get('accuracy', 'N/A')}\")\n",
    "    print()\n",
    "\n",
    "# Load the best model based on F1-score\n",
    "print('\\nLoading best performing model...')\n",
    "best_model_result = registry.get_best_model(metric='f1_score')\n",
    "\n",
    "if best_model_result is None:\n",
    "    raise ValueError(\"No models found in registry. Please run the training notebook first.\")\n",
    "\n",
    "best_model, best_metadata = best_model_result\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('Best Model Loaded:')\n",
    "print('=' * 80)\n",
    "print(f\"Model Name: {best_metadata.get('model_name', 'Unknown')}\")\n",
    "print(f\"Model Type: {best_metadata.get('model_type', 'Unknown')}\")\n",
    "print(f\"Version: {best_metadata.get('version', 'Unknown')}\")\n",
    "print(f\"Training Date: {best_metadata.get('training_date', 'Unknown')}\")\n",
    "print(f\"\\nCurrent Performance Metrics:\")\n",
    "if 'metrics' in best_metadata:\n",
    "    for metric, value in best_metadata['metrics'].items():\n",
    "        if value is not None:\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Hyperparameter Grids from Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which model to optimize based on loaded best model\n",
    "model_type = best_metadata.get('model_type', '')\n",
    "original_model_name = best_metadata.get('model_name', '')\n",
    "\n",
    "print(f'Model type to optimize: {model_type}')\n",
    "print(f'Original model name: {original_model_name}')\n",
    "\n",
    "# Get hyperparameter grid from config\n",
    "param_grids = config['hyperparameter_tuning']['param_grids']\n",
    "\n",
    "# Determine which parameter grid to use\n",
    "if 'RandomForest' in model_type or 'random_forest' in original_model_name:\n",
    "    param_grid = param_grids['random_forest']\n",
    "    base_model = RandomForestClassifier(random_state=config['data']['random_state'])\n",
    "    model_name_for_optimization = 'random_forest'\n",
    "    print('\\nUsing Random Forest hyperparameter grid')\n",
    "elif 'XGB' in model_type or 'xgboost' in original_model_name:\n",
    "    param_grid = param_grids['xgboost']\n",
    "    base_model = XGBClassifier(random_state=config['data']['random_state'], eval_metric='logloss')\n",
    "    model_name_for_optimization = 'xgboost'\n",
    "    print('\\nUsing XGBoost hyperparameter grid')\n",
    "else:\n",
    "    # Default to random forest if model type is unclear\n",
    "    print(f'\\nWarning: Could not determine model type from {model_type}. Defaulting to Random Forest.')\n",
    "    param_grid = param_grids['random_forest']\n",
    "    base_model = RandomForestClassifier(random_state=config['data']['random_state'])\n",
    "    model_name_for_optimization = 'random_forest'\n",
    "\n",
    "print('\\nHyperparameter Grid:')\n",
    "print('=' * 80)\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize HyperparameterOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HyperparameterOptimizer with configuration\n",
    "optimizer = HyperparameterOptimizer(\n",
    "    param_grid=param_grid,\n",
    "    search_method=config['hyperparameter_tuning']['method'],\n",
    "    cv_folds=config['hyperparameter_tuning']['cv_folds'],\n",
    "    n_iter=config['hyperparameter_tuning']['n_iter'],\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print('HyperparameterOptimizer initialized successfully!')\n",
    "print(f\"Search method: {config['hyperparameter_tuning']['method']}\")\n",
    "print(f\"CV folds: {config['hyperparameter_tuning']['cv_folds']}\")\n",
    "print(f\"Scoring metric: f1_weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Hyperparameter Optimization (Limited to 2 Hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "optimization_start_time = time.time()\n",
    "max_optimization_time = 2 * 60 * 60  # 2 hours in seconds\n",
    "\n",
    "print('Starting hyperparameter optimization...')\n",
    "print('=' * 80)\n",
    "print(f'Maximum optimization time: 2 hours')\n",
    "print(f'Start time: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print('=' * 80)\n",
    "print('\\nThis may take a while. Progress will be displayed below...\\n')\n",
    "\n",
    "# Run optimization\n",
    "search_result = optimizer.optimize(\n",
    "    model=base_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Calculate optimization time\n",
    "optimization_time = time.time() - optimization_start_time\n",
    "optimization_hours = optimization_time / 3600\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('Hyperparameter Optimization Completed!')\n",
    "print('=' * 80)\n",
    "print(f'Total optimization time: {optimization_hours:.2f} hours ({optimization_time:.2f} seconds)')\n",
    "print(f'End time: {time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "# Check if optimization exceeded time limit\n",
    "if optimization_time > max_optimization_time:\n",
    "    print(f'\\n\u26a0\ufe0f  Warning: Optimization exceeded 2-hour limit by {(optimization_time - max_optimization_time)/60:.2f} minutes')\n",
    "else:\n",
    "    print(f'\\n\u2713 Optimization completed within time limit')\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Display Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best parameters and score\n",
    "best_params = optimizer.get_best_params()\n",
    "best_cv_score = optimizer.get_best_score()\n",
    "optimized_model = optimizer.get_best_estimator()\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('OPTIMIZATION RESULTS')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\nBest Cross-Validation F1-Score: {best_cv_score:.4f}')\n",
    "\n",
    "print('\\nBest Hyperparameters:')\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Compare with original model performance\n",
    "if 'metrics' in best_metadata and 'f1_score' in best_metadata['metrics']:\n",
    "    original_f1 = best_metadata['metrics']['f1_score']\n",
    "    improvement = best_cv_score - original_f1\n",
    "    improvement_pct = (improvement / original_f1) * 100\n",
    "    \n",
    "    print(f'\\nComparison with Original Model:')\n",
    "    print(f\"  Original F1-Score: {original_f1:.4f}\")\n",
    "    print(f\"  Optimized CV F1-Score: {best_cv_score:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CV results\n",
    "cv_results = optimizer.get_cv_results()\n",
    "\n",
    "# Create visualization of parameter search\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Score distribution\n",
    "ax1 = axes[0]\n",
    "scores = cv_results['mean_test_score']\n",
    "ax1.hist(scores, bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(best_cv_score, color='red', linestyle='--', linewidth=2, label=f'Best Score: {best_cv_score:.4f}')\n",
    "ax1.set_xlabel('F1-Score', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Distribution of Cross-Validation Scores', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Top 10 parameter combinations\n",
    "ax2 = axes[1]\n",
    "top_n = min(10, len(scores))\n",
    "top_indices = scores.argsort()[-top_n:][::-1]\n",
    "top_scores = scores[top_indices]\n",
    "top_ranks = range(1, top_n + 1)\n",
    "\n",
    "bars = ax2.barh(top_ranks, top_scores, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('F1-Score', fontsize=12)\n",
    "ax2.set_ylabel('Rank', fontsize=12)\n",
    "ax2.set_title(f'Top {top_n} Parameter Combinations', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add score labels\n",
    "for i, (rank, score) in enumerate(zip(top_ranks, top_scores)):\n",
    "    ax2.text(score + 0.001, rank, f'{score:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/hyperparameter_optimization_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Optimization visualization saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate Optimized Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ModelEvaluator\n",
    "evaluator = ModelEvaluator(output_dir='../reports/figures')\n",
    "\n",
    "print('Evaluating optimized model on test set...')\n",
    "print('=' * 80)\n",
    "\n",
    "# Evaluate optimized model\n",
    "optimized_metrics = evaluator.evaluate_model(\n",
    "    model=optimized_model,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    model_name=f'{model_name_for_optimization}_optimized'\n",
    ")\n",
    "\n",
    "# Plot confusion matrix\n",
    "evaluator.plot_confusion_matrix_heatmap(\n",
    "    y_test,\n",
    "    optimized_model.predict(X_test),\n",
    "    model_name=f'{model_name_for_optimization}_optimized'\n",
    ")\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('OPTIMIZED MODEL TEST SET PERFORMANCE')\n",
    "print('=' * 80)\n",
    "\n",
    "# Display metrics\n",
    "metrics_clean = {k: v for k, v in optimized_metrics.items() if k != 'confusion_matrix'}\n",
    "for metric, value in metrics_clean.items():\n",
    "    if value is not None:\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verify F1-Score Improvement and Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimum F1-score threshold\n",
    "min_f1_threshold = 0.80\n",
    "optimized_f1 = optimized_metrics['f1_score']\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('F1-SCORE VERIFICATION')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\nMinimum F1-Score Threshold: {min_f1_threshold:.2f}')\n",
    "print(f'Optimized Model F1-Score: {optimized_f1:.4f}')\n",
    "\n",
    "# Check if threshold is met\n",
    "if optimized_f1 >= min_f1_threshold:\n",
    "    print(f'\\n\u2705 SUCCESS: F1-score meets minimum threshold!')\n",
    "    print(f'   Exceeds threshold by: {(optimized_f1 - min_f1_threshold):.4f}')\n",
    "    threshold_met = True\n",
    "else:\n",
    "    print(f'\\n\u26a0\ufe0f  WARNING: F1-score below minimum threshold')\n",
    "    print(f'   Falls short by: {(min_f1_threshold - optimized_f1):.4f}')\n",
    "    threshold_met = False\n",
    "\n",
    "# Compare with original model\n",
    "if 'metrics' in best_metadata and 'f1_score' in best_metadata['metrics']:\n",
    "    original_f1 = best_metadata['metrics']['f1_score']\n",
    "    f1_improvement = optimized_f1 - original_f1\n",
    "    f1_improvement_pct = (f1_improvement / original_f1) * 100\n",
    "    \n",
    "    print(f'\\nComparison with Original Model:')\n",
    "    print(f\"  Original Test F1-Score: {original_f1:.4f}\")\n",
    "    print(f\"  Optimized Test F1-Score: {optimized_f1:.4f}\")\n",
    "    print(f\"  Improvement: {f1_improvement:+.4f} ({f1_improvement_pct:+.2f}%)\")\n",
    "    \n",
    "    if f1_improvement > 0:\n",
    "        print(f'\\n\u2705 Model performance improved after optimization!')\n",
    "    elif f1_improvement == 0:\n",
    "        print(f'\\n\u27a1\ufe0f  Model performance unchanged after optimization')\n",
    "    else:\n",
    "        print(f'\\n\u26a0\ufe0f  Model performance decreased after optimization')\n",
    "\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Optimized Model as best_model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metadata for optimized model\n",
    "optimized_metadata = {\n",
    "    'model_name': 'best_model',\n",
    "    'version': '2.0.0',\n",
    "    'original_model_name': model_name_for_optimization,\n",
    "    'optimization_method': config['hyperparameter_tuning']['method'],\n",
    "    'cv_folds': config['hyperparameter_tuning']['cv_folds'],\n",
    "    'optimization_time_hours': optimization_hours,\n",
    "    'best_cv_score': best_cv_score,\n",
    "    'threshold_met': threshold_met,\n",
    "    'min_f1_threshold': min_f1_threshold\n",
    "}\n",
    "\n",
    "# Clean metrics (remove confusion matrix)\n",
    "metrics_to_save = {k: v for k, v in optimized_metrics.items() if k != 'confusion_matrix'}\n",
    "\n",
    "# Save optimized model\n",
    "print('\\nSaving optimized model...')\n",
    "print('=' * 80)\n",
    "\n",
    "optimized_model_path = registry.save_model(\n",
    "    model=optimized_model,\n",
    "    model_name='best_model',\n",
    "    version='2.0.0',\n",
    "    metrics=metrics_to_save,\n",
    "    hyperparameters=best_params,\n",
    "    additional_metadata=optimized_metadata\n",
    ")\n",
    "\n",
    "print(f'\\n\u2705 Optimized model saved successfully!')\n",
    "print(f'   Path: {optimized_model_path}')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '=' * 100)\n",
    "print('HYPERPARAMETER OPTIMIZATION SUMMARY')\n",
    "print('=' * 100)\n",
    "\n",
    "print(f'\\n\ud83d\udcca Optimization Details:')\n",
    "print(f'   - Model Type: {model_type}')\n",
    "print(f'   - Search Method: {config[\"hyperparameter_tuning\"][\"method\"]}')\n",
    "print(f'   - CV Folds: {config[\"hyperparameter_tuning\"][\"cv_folds\"]}')\n",
    "print(f'   - Iterations: {config[\"hyperparameter_tuning\"][\"n_iter\"]}')\n",
    "print(f'   - Optimization Time: {optimization_hours:.2f} hours')\n",
    "\n",
    "print(f'\\n\ud83c\udfaf Performance Results:')\n",
    "print(f'   - Best CV F1-Score: {best_cv_score:.4f}')\n",
    "print(f'   - Test Set Accuracy: {optimized_metrics[\"accuracy\"]:.4f}')\n",
    "print(f'   - Test Set Precision: {optimized_metrics[\"precision\"]:.4f}')\n",
    "print(f'   - Test Set Recall: {optimized_metrics[\"recall\"]:.4f}')\n",
    "print(f'   - Test Set F1-Score: {optimized_metrics[\"f1_score\"]:.4f}')\n",
    "if optimized_metrics['roc_auc'] is not None:\n",
    "    print(f'   - Test Set ROC-AUC: {optimized_metrics[\"roc_auc\"]:.4f}')\n",
    "\n",
    "print(f'\\n\ud83d\udd27 Best Hyperparameters:')\n",
    "for param, value in best_params.items():\n",
    "    print(f'   - {param}: {value}')\n",
    "\n",
    "print(f'\\n\u2705 Requirements Verification:')\n",
    "print(f'   - Requirement 6.1: Hyperparameter tuning performed \u2713')\n",
    "print(f'   - Requirement 6.2: Multiple hyperparameters optimized \u2713')\n",
    "print(f'   - Requirement 6.3: Cross-validation used during search \u2713')\n",
    "print(f'   - Requirement 6.4: Optimization completed within 2 hours: {\"\u2713\" if optimization_time <= max_optimization_time else \"\u2717\"}')\n",
    "print(f'   - Requirement 6.5: F1-score threshold (0.80) met: {\"\u2713\" if threshold_met else \"\u2717\"}')\n",
    "\n",
    "if threshold_met:\n",
    "    print(f'\\n\ud83c\udf89 SUCCESS: All requirements met!')\n",
    "    print(f'   The optimized model achieves F1-score of {optimized_f1:.4f}, exceeding the minimum threshold of {min_f1_threshold:.2f}')\n",
    "else:\n",
    "    print(f'\\n\u26a0\ufe0f  Note: F1-score threshold not met')\n",
    "    print(f'   Current F1-score: {optimized_f1:.4f}')\n",
    "    print(f'   Required threshold: {min_f1_threshold:.2f}')\n",
    "    print(f'   Consider: Collecting more data, feature engineering, or trying different models')\n",
    "\n",
    "print(f'\\n\ud83d\udcbe Model Saved:')\n",
    "print(f'   - File: best_model_v2.0.0.pkl')\n",
    "print(f'   - Location: {optimized_model_path}')\n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('Hyperparameter optimization notebook completed successfully!')\n",
    "print('=' * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}